{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Download pippa.jsonl using wget\n!wget -O /kaggle/working/pippa.jsonl \"https://huggingface.co/datasets/PygmalionAI/PIPPA/resolve/main/pippa.jsonl?download=true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T11:30:18.772692Z","iopub.execute_input":"2025-05-21T11:30:18.772915Z","iopub.status.idle":"2025-05-21T11:30:29.465820Z","shell.execute_reply.started":"2025-05-21T11:30:18.772899Z","shell.execute_reply":"2025-05-21T11:30:29.465177Z"}},"outputs":[{"name":"stdout","text":"--2025-05-21 11:30:18--  https://huggingface.co/datasets/PygmalionAI/PIPPA/resolve/main/pippa.jsonl?download=true\nResolving huggingface.co (huggingface.co)... 3.169.137.19, 3.169.137.111, 3.169.137.119, ...\nConnecting to huggingface.co (huggingface.co)|3.169.137.19|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://cas-bridge.xethub.hf.co/xet-bridge-us/64d19b3873dc458c1658fa1e/406cf96ad5700c7616998c15f7abf19738e5013c560df2094999f458ac54c4ee?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250521%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250521T113018Z&X-Amz-Expires=3600&X-Amz-Signature=453b210c10a9957eff4956e0423f49a12d80f9ad2bee75b4e67a4f2adbb44ebf&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pippa.jsonl%3B+filename%3D%22pippa.jsonl%22%3B&x-id=GetObject&Expires=1747830618&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NzgzMDYxOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NGQxOWIzODczZGM0NThjMTY1OGZhMWUvNDA2Y2Y5NmFkNTcwMGM3NjE2OTk4YzE1ZjdhYmYxOTczOGU1MDEzYzU2MGRmMjA5NDk5OWY0NThhYzU0YzRlZSoifV19&Signature=aCwMo-Y0CCtjdx1JvsgIo6tcoUW-WBWDFg4swEm2s9fwB3ZtN9rsKQjOpm%7EWQV0qtVaG2CLJmMXR04hFcmU6CJE5KGNnOZyZmDSrznlTCX9HRfpwyOC1eGp8aqU26NP5m-Q%7EwhTB37-jVuDhELk-Vvj37UqPiGW5qGuwMzZi4u3wuY04Hne6HQ5gVtIvfEiTvl%7ETlkijQFbCp1k4zSeNlaZl5Kx8xrh8QxcVe1yWJtfQOcmKMV9coRkKLOfFSf%7ESuh9WN0nFYmibHQNJEHWWzziKUGHcBdrhHNOBWK4qgethYN4yl4s6OpSefXTzXR7joco%7EY2Xz00YtNVJPz1Xy-g__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n--2025-05-21 11:30:19--  https://cas-bridge.xethub.hf.co/xet-bridge-us/64d19b3873dc458c1658fa1e/406cf96ad5700c7616998c15f7abf19738e5013c560df2094999f458ac54c4ee?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250521%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250521T113018Z&X-Amz-Expires=3600&X-Amz-Signature=453b210c10a9957eff4956e0423f49a12d80f9ad2bee75b4e67a4f2adbb44ebf&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pippa.jsonl%3B+filename%3D%22pippa.jsonl%22%3B&x-id=GetObject&Expires=1747830618&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NzgzMDYxOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NGQxOWIzODczZGM0NThjMTY1OGZhMWUvNDA2Y2Y5NmFkNTcwMGM3NjE2OTk4YzE1ZjdhYmYxOTczOGU1MDEzYzU2MGRmMjA5NDk5OWY0NThhYzU0YzRlZSoifV19&Signature=aCwMo-Y0CCtjdx1JvsgIo6tcoUW-WBWDFg4swEm2s9fwB3ZtN9rsKQjOpm%7EWQV0qtVaG2CLJmMXR04hFcmU6CJE5KGNnOZyZmDSrznlTCX9HRfpwyOC1eGp8aqU26NP5m-Q%7EwhTB37-jVuDhELk-Vvj37UqPiGW5qGuwMzZi4u3wuY04Hne6HQ5gVtIvfEiTvl%7ETlkijQFbCp1k4zSeNlaZl5Kx8xrh8QxcVe1yWJtfQOcmKMV9coRkKLOfFSf%7ESuh9WN0nFYmibHQNJEHWWzziKUGHcBdrhHNOBWK4qgethYN4yl4s6OpSefXTzXR7joco%7EY2Xz00YtNVJPz1Xy-g__&Key-Pair-Id=K2L8F4GPSG1IFC\nResolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 3.169.55.68, 3.169.55.31, 3.169.55.121, ...\nConnecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|3.169.55.68|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 348589691 (332M)\nSaving to: ‘/kaggle/working/pippa.jsonl’\n\n/kaggle/working/pip 100%[===================>] 332.44M  33.9MB/s    in 10s     \n\n2025-05-21 11:30:29 (33.1 MB/s) - ‘/kaggle/working/pippa.jsonl’ saved [348589691/348589691]\n\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport json\nimport re\nfrom datasets import Dataset, DatasetDict\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom torch.optim import AdamW\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import get_cosine_schedule_with_warmup, DataCollatorForLanguageModeling\nimport logging\nfrom retry import retry\nimport shutil\nimport numpy as np\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Configurable paths\nDATASET_PATH = os.getenv(\"DATASET_PATH\", \"/kaggle/working/pippa.jsonl\")\nSAVE_DIR = os.getenv(\"SAVE_DIR\", \"/kaggle/working/conversational_ai_v1\")\nSAVE_ZIP = os.getenv(\"SAVE_ZIP\", \"/kaggle/working/conversational_ai_v1.zip\")\nDRIVE_SAVE_PATH = os.getenv(\"DRIVE_SAVE_PATH\", \"/content/drive/My Drive/conversational_ai_v1.zip\")\nBEST_MODEL_DIR = os.getenv(\"BEST_MODEL_DIR\", \"/kaggle/working/conversational_ai_v1_best\")\n\n# Training hyperparameters\nBATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", 4))\nEPOCHS = int(os.getenv(\"EPOCHS\", 3))\nACCUMULATION_STEPS = int(os.getenv(\"ACCUMULATION_STEPS\", 4))\nLEARNING_RATE = float(os.getenv(\"LEARNING_RATE\", 5e-5))\nMAX_LENGTH = int(os.getenv(\"MAX_LENGTH\", 512))\nMAX_CONVERSATIONS = int(os.getenv(\"MAX_CONVERSATIONS\", 10000))\nWARMUP_STEPS = int(os.getenv(\"WARMUP_STEPS\", 1000))\nEARLY_STOPPING_PATIENCE = int(os.getenv(\"EARLY_STOPPING_PATIENCE\", 1))\n\n# Mount Google Drive (optional)\ntry:\n    from google.colab import drive\n    drive.mount('/content/drive')\n    logger.info(\"Google Drive mounted for saving outputs.\")\nexcept Exception:\n    logger.warning(\"Google Drive not mounted; saving outputs to /kaggle/working/\")\n\n# Check dataset\nif not os.path.exists(DATASET_PATH):\n    logger.error(f\"Dataset '{DATASET_PATH}' not found.\")\n    raise FileNotFoundError(\n        f\"Dataset '{DATASET_PATH}' not found. Ensure 'pippa.jsonl' is downloaded to /kaggle/working/pippa.jsonl.\"\n    )\n\n# Load tokenizer and model\ntry:\n    tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n    tokenizer.pad_token = tokenizer.eos_token\n    model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\nexcept Exception as e:\n    logger.error(f\"Failed to load model or tokenizer: {e}\")\n    raise\n\n# Set device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\nlogger.info(f\"Using device: {device}\")\nif device == \"cpu\":\n    logger.warning(\"GPU unavailable; using CPU. Ensure Accelerator is set to 'GPU P100' in Kaggle settings.\")\n\n# Check GPU memory\nif device == \"cuda\":\n    logger.info(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n    logger.info(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n\n# Optimizer and scaler\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\nscaler = torch.amp.GradScaler('cuda')\n\n# Learning rate scheduler\ntotal_steps = (MAX_CONVERSATIONS // (BATCH_SIZE * ACCUMULATION_STEPS)) * EPOCHS\nscheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=total_steps)\n\n# System prompt\nSYSTEM_PROMPT = (\n    \"[SYSTEM]: You are a conversational AI assistant designed to provide helpful and accurate responses. \"\n    \"Respond in a formal, polite, and professional manner, suitable for general conversation.\"\n)\n\n# Clean response\ndef clean_response(text):\n    text = re.sub(r'\\{\\{.*?\\}\\}', 'User', text)\n    text = re.sub(r'\\*.*?\\*', '', text)\n    text = text[0].upper() + text[1:] if text else text\n    text = text.rstrip('.') + '.' if not text.endswith('.') else text\n    return text.strip()\n\n# Preprocess dataset with content filter\ndef process_line(line):\n    try:\n        data = json.loads(line)\n        conversation = data.get(\"conversation\", [])\n        if not conversation or len(conversation) < 2:\n            logger.warning(\"Skipping empty or short conversation\")\n            return None\n        sensitive_keywords = ['explicit', 'adult', 'nsfw', 'offensive']\n        for msg in conversation:\n            if not isinstance(msg.get('message'), str):\n                logger.warning(\"Skipping conversation with invalid message type\")\n                return None\n            if any(keyword in msg['message'].lower() for keyword in sensitive_keywords):\n                logger.warning(\"Skipping conversation with sensitive content\")\n                return None\n        text = SYSTEM_PROMPT + \" \"\n        for msg in conversation:\n            speaker = \"[USER]\" if msg[\"is_human\"] else \"[BOT]\"\n            message = clean_response(msg['message'])\n            text += f\"{speaker}: {message} \"\n        tokenized = tokenizer(text, truncation=True, max_length=MAX_LENGTH)\n        if len(tokenized[\"input_ids\"]) < 50:\n            logger.warning(\"Skipping conversation with insufficient tokens\")\n            return None\n        return {\"text\": text.strip()}\n    except json.JSONDecodeError:\n        logger.warning(\"Skipping invalid JSON line\")\n        return None\n    except Exception as e:\n        logger.warning(f\"Skipping line due to error: {e}\")\n        return None\n\n# Load dataset with limit\ndef load_dataset_chunked(file_path, chunk_size=1000, max_conversations=MAX_CONVERSATIONS):\n    texts = []\n    count = 0\n    try:\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                if line.strip():\n                    processed = process_line(line)\n                    if processed:\n                        texts.append(processed)\n                        count += 1\n                        if count >= max_conversations:\n                            break\n                if len(texts) >= chunk_size or count >= max_conversations:\n                    yield texts\n                    texts = []\n            if texts:\n                yield texts\n        logger.info(f\"Processed {count} valid conversations\")\n    except Exception as e:\n        logger.error(f\"Failed to load dataset: {e}\")\n        raise\n\n# Load and split dataset\nlogger.info(\"Loading and preprocessing dataset...\")\ntry:\n    chunks = list(load_dataset_chunked(DATASET_PATH))\n    dataset = Dataset.from_list([item for chunk in chunks for item in chunk if item])\n    dataset.save_to_disk(\"/kaggle/working/preprocessed_dataset\")\n    train_test_split = dataset.train_test_split(test_size=0.1)\n    dataset_dict = DatasetDict({\"train\": train_test_split[\"train\"], \"validation\": train_test_split[\"test\"]})\nexcept Exception as e:\n    logger.error(f\"Dataset preprocessing failed: {e}\")\n    raise\n\n# Tokenize dataset with explicit padding\ndef tokenize_function(examples):\n    try:\n        tokenized = tokenizer(\n            examples[\"text\"],\n            truncation=True,\n            padding=\"max_length\",\n            max_length=MAX_LENGTH,\n            return_tensors=\"pt\"\n        )\n        tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n        return {\n            \"input_ids\": tokenized[\"input_ids\"].squeeze(),\n            \"attention_mask\": tokenized[\"attention_mask\"].squeeze(),\n            \"labels\": tokenized[\"labels\"].squeeze()\n        }\n    except Exception as e:\n        logger.error(f\"Tokenization failed: {e}\")\n        raise\n\ntry:\n    tokenized_datasets = dataset_dict.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n    tokenized_datasets.save_to_disk(\"/kaggle/working/tokenized_dataset\")\n    logger.info(\"Dataset tokenized and cached.\")\nexcept Exception as e:\n    logger.error(f\"Tokenization failed: {e}\")\n    raise\n\n# Custom collator for dynamic padding\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n# DataLoader\ndef create_dataloader(dataset, batch_size=BATCH_SIZE):\n    dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n    return DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n\ntry:\n    train_dataloader = create_dataloader(tokenized_datasets[\"train\"])\n    val_dataloader = create_dataloader(tokenized_datasets[\"validation\"])\nexcept Exception as e:\n    logger.error(f\"DataLoader creation failed: {e}\")\n    raise\n\n# Save checkpoint\n@retry(tries=3, delay=2, backoff=2)\ndef save_checkpoint(epoch, is_best=False):\n    checkpoint_dir = BEST_MODEL_DIR if is_best else f\"{SAVE_DIR}_epoch_{epoch}\"\n    try:\n        model.save_pretrained(checkpoint_dir)\n        tokenizer.save_pretrained(checkpoint_dir)\n        shutil.make_archive(checkpoint_dir, 'zip', checkpoint_dir)\n        try:\n            drive_path = f\"/content/drive/My Drive/{os.path.basename(checkpoint_dir)}.zip\"\n            if os.path.exists(drive_path):\n                os.remove(drive_path)\n                logger.info(f\"Deleted existing checkpoint at '{drive_path}'\")\n            shutil.copy(f\"{checkpoint_dir}.zip\", drive_path)\n            logger.info(f\"Saved checkpoint at '{checkpoint_dir}' and uploaded to '{drive_path}'\")\n        except Exception:\n            logger.warning(f\"Drive save failed for checkpoint; saved locally at '{checkpoint_dir}.zip'\")\n    except Exception as e:\n        logger.error(f\"Checkpoint saving failed: {e}\")\n        raise\n\n# Training function with early stopping\ndef train_model(epochs=EPOCHS, accumulation_steps=ACCUMULATION_STEPS):\n    model.train()\n    global grad_step_counter\n    grad_step_counter = 0\n    train_losses = []\n    val_losses = []\n    best_val_loss = float('inf')\n    patience_counter = 0\n\n    for epoch in range(epochs):\n        epoch_loss = 0\n        batch_count = 0\n        try:\n            for batch in train_dataloader:\n                inputs = {k: v.to(device) for k, v in batch.items()}\n                with torch.amp.autocast('cuda'):\n                    outputs = model(**inputs)\n                    loss = outputs.loss / accumulation_steps\n\n                scaler.scale(loss).backward()\n                grad_step_counter += 1\n                if grad_step_counter % accumulation_steps == 0:\n                    scaler.step(optimizer)\n                    scaler.update()\n                    optimizer.zero_grad()\n                    scheduler.step()\n\n                epoch_loss += loss.item() * accumulation_steps\n                batch_count += 1\n                if device == \"cuda\":\n                    torch.cuda.empty_cache()\n\n            avg_train_loss = epoch_loss / batch_count\n            train_losses.append(avg_train_loss)\n\n            # Validation\n            model.eval()\n            val_loss = 0\n            val_count = 0\n            with torch.no_grad():\n                for batch in val_dataloader:\n                    inputs = {k: v.to(device) for k, v in batch.items()}\n                    outputs = model(**inputs)\n                    val_loss += outputs.loss.item()\n                    val_count += 1\n            avg_val_loss = val_loss / val_count\n            val_losses.append(avg_val_loss)\n            perplexity = torch.exp(torch.tensor(avg_val_loss)).item()\n\n            logger.info(\n                f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}, \"\n                f\"Val Loss: {avg_val_loss:.4f}, Perplexity: {perplexity:.2f}\"\n            )\n\n            # Save checkpoint\n            save_checkpoint(epoch + 1)\n\n            # Save best model\n            if avg_val_loss < best_val_loss:\n                best_val_loss = avg_val_loss\n                save_checkpoint(epoch + 1, is_best=True)\n                logger.info(f\"New best model saved with val loss: {best_val_loss:.4f}\")\n                patience_counter = 0\n            else:\n                patience_counter += 1\n                logger.info(f\"No improvement in val loss. Patience: {patience_counter}/{EARLY_STOPPING_PATIENCE}\")\n\n            # Early stopping\n            if patience_counter >= EARLY_STOPPING_PATIENCE:\n                logger.info(\"Early stopping triggered.\")\n                break\n\n            if device == \"cuda\":\n                logger.info(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n\n            model.train()\n        except Exception as e:\n            logger.error(f\"Training failed at epoch {epoch+1}: {e}\")\n            raise\n\n    return train_losses, val_losses\n\n# Run training\nlogger.info(\"Starting training...\")\ntry:\n    train_losses, val_losses = train_model()\n    logger.info(\"Training completed.\")\nexcept Exception as e:\n    logger.error(f\"Training failed: {e}\")\n    raise\n\n# Save final model\ntry:\n    model.save_pretrained(SAVE_DIR)\n    tokenizer.save_pretrained(SAVE_DIR)\n    shutil.make_archive(SAVE_DIR, 'zip', SAVE_DIR)\n    logger.info(f\"Saved '{SAVE_ZIP}'\")\nexcept Exception as e:\n    logger.error(f\"Final model save failed: {e}\")\n    raise\n\n# Upload to Drive (optional)\n@retry(tries=3, delay=2, backoff=2)\ndef save_to_drive():\n    try:\n        if os.path.exists(DRIVE_SAVE_PATH):\n            os.remove(DRIVE_SAVE_PATH)\n            logger.info(f\"Deleted existing file at '{DRIVE_SAVE_PATH}'\")\n        shutil.copy(f\"{SAVE_ZIP}\", DRIVE_SAVE_PATH)\n        logger.info(f\"Copied '{SAVE_ZIP}' to '{DRIVE_SAVE_PATH}'\")\n    except Exception as e:\n        logger.warning(f\"Drive save failed; outputs saved to /kaggle/working/\")\n\ntry:\n    save_to_drive()\nexcept Exception:\n    logger.warning(\"Drive save failed; outputs saved to /kaggle/working/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T11:31:46.354228Z","iopub.execute_input":"2025-05-21T11:31:46.354759Z","iopub.status.idle":"2025-05-21T12:04:27.365675Z","shell.execute_reply.started":"2025-05-21T11:31:46.354731Z","shell.execute_reply":"2025-05-21T12:04:27.365085Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e0ec3726ad649f69a0d193d0712f36d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"149ce7b39cc248e3a225f2b3c13e4267"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fafdb3c050a64f2a8c5109d6c6235e38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8824e2f673d4e6594f06420bace8d12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"141b232a8aec49ee97f985599283cf3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2092b420611546ee97729ac9eed1e847"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2c4678dde6842eca51ee2245dc3be82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93446d65707e48b8aac9a07aa81013b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc9ea38b95944a1e8b1c93cd914079c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a1c74df216843099cadedc500440dac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/9000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d32dce9da47479da24f98d4e8aa8dc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94630a335a5948508a620e97415ff629"}},"metadata":{}},{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport re\nimport torch\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nimport logging\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Configurable paths\nMODEL_DIR = os.getenv(\"MODEL_DIR\", \"/kaggle/working/conversational_ai_v1\")\n\n# System prompt (same as training)\nSYSTEM_PROMPT = (\n    \"[SYSTEM]: You are a conversational AI assistant designed to provide helpful and accurate responses. \"\n    \"Respond in a formal, polite, and professional manner, suitable for general conversation.\"\n)\n\n# Clean response (same as training)\ndef clean_response(text):\n    text = re.sub(r'\\{\\{.*?\\}\\}', 'User', text)\n    text = re.sub(r'\\*.*?\\*', '', text)\n    text = text[0].upper() + text[1:] if text else text\n    text = text.rstrip('.') + '.' if not text.endswith('.') else text\n    return text.strip()\n\n# Load model and tokenizer\ndef load_model_and_tokenizer(model_dir):\n    try:\n        logger.info(f\"Loading tokenizer from {model_dir}\")\n        tokenizer = GPT2Tokenizer.from_pretrained(model_dir)\n        logger.info(f\"Loading model from {model_dir}\")\n        model = GPT2LMHeadModel.from_pretrained(model_dir)\n        return tokenizer, model\n    except Exception as e:\n        logger.error(f\"Failed to load model or tokenizer: {e}\")\n        raise\n\n# Generate response\ndef generate_response(model, tokenizer, user_input, device, max_length=256, max_new_tokens=100):\n    try:\n        # Format input with system prompt and user message\n        input_text = f\"{SYSTEM_PROMPT} [USER]: {user_input.strip()} [BOT]: \"\n        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=max_length).to(device)\n        \n        # Generate\n        outputs = model.generate(\n            input_ids=inputs[\"input_ids\"],\n            attention_mask=inputs[\"attention_mask\"],\n            max_new_tokens=max_new_tokens,\n            num_beams=5,\n            no_repeat_ngram_size=2,\n            pad_token_id=tokenizer.eos_token_id,\n            do_sample=True,\n            top_p=0.9,\n            temperature=0.7\n        )\n        \n        # Decode response\n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n        # Extract BOT response (after [BOT]:)\n        bot_response = response.split(\"[BOT]:\")[-1].strip()\n        bot_response = clean_response(bot_response)\n        \n        return bot_response\n    except Exception as e:\n        logger.error(f\"Error generating response: {e}\")\n        return \"I'm sorry, I encountered an error. Please try again.\"\n\n# Interactive chat loop\ndef chat_loop(model, tokenizer, device):\n    logger.info(\"Starting chat. Type 'exit' to quit.\")\n    while True:\n        user_input = input(\"You: \")\n        if user_input.lower() == 'exit':\n            logger.info(\"Exiting chat.\")\n            break\n        response = generate_response(model, tokenizer, user_input, device)\n        print(f\"Bot: {response}\")\n\n# Main function\ndef main():\n    # Verify model directory\n    if not os.path.exists(MODEL_DIR):\n        logger.error(f\"Model directory '{MODEL_DIR}' not found.\")\n        raise FileNotFoundError(\n            f\"Model directory '{MODEL_DIR}' not found. Ensure 'conversational_ai_v1.zip' is unzipped.\"\n        )\n\n    # Load model and tokenizer\n    tokenizer, model = load_model_and_tokenizer(MODEL_DIR)\n\n    # Set device\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model.to(device)\n    logger.info(f\"Using device: {device}\")\n    if device == \"cuda\":\n        logger.info(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n    else:\n        logger.warning(\"GPU unavailable; using CPU. Responses may be slower.\")\n\n    # Start chat\n    try:\n        chat_loop(model, tokenizer, device)\n    except KeyboardInterrupt:\n        logger.info(\"Chat interrupted by user.\")\n    except Exception as e:\n        logger.error(f\"Chat loop failed: {e}\")\n        raise\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T13:54:30.378552Z","iopub.execute_input":"2025-05-21T13:54:30.378821Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"You:  Hey there, how are you doing?\n"},{"name":"stdout","text":"Bot: I've been working hard to become the best I can be, but I'm still struggling to make it to the top of my game. I've had a lot of work to do, so I feel like I should be more than happy to be part of the team. It's been a long time since I started working on this game, I don't think I'll ever get the chance to play again. \n\n\nSo, what do you think of me? Do you have.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"!pip install retry","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T11:31:34.792032Z","iopub.execute_input":"2025-05-21T11:31:34.792845Z","iopub.status.idle":"2025-05-21T11:31:38.842979Z","shell.execute_reply.started":"2025-05-21T11:31:34.792813Z","shell.execute_reply":"2025-05-21T11:31:38.842023Z"}},"outputs":[{"name":"stdout","text":"Collecting retry\n  Downloading retry-0.9.2-py2.py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: decorator>=3.4.2 in /usr/local/lib/python3.11/dist-packages (from retry) (4.4.2)\nCollecting py<2.0.0,>=1.4.26 (from retry)\n  Downloading py-1.11.0-py2.py3-none-any.whl.metadata (2.8 kB)\nDownloading retry-0.9.2-py2.py3-none-any.whl (8.0 kB)\nDownloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: py, retry\nSuccessfully installed py-1.11.0 retry-0.9.2\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}